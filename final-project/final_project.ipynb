{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>204126.52</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>197426.42</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>144267.27</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>151903.00</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>117399.88</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id  store_name       date    revenue     country       store event\n",
       "0         0  All Stores 2011-01-29  204126.52  All Stores  All Stores   NaN\n",
       "1         0  All Stores 2011-01-30  197426.42  All Stores  All Stores   NaN\n",
       "2         0  All Stores 2011-01-31  144267.27  All Stores  All Stores   NaN\n",
       "3         0  All Stores 2011-02-01  151903.00  All Stores  All Stores   NaN\n",
       "4         0  All Stores 2011-02-02  117399.88  All Stores  All Stores   NaN"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('train.csv', parse_dates=['date'])\n",
    "events = pd.read_csv('calendar_events.csv', parse_dates=['date'])\n",
    "\n",
    "#PREPROCESS\n",
    "#Extract store name\n",
    "def parse_store_name(df):\n",
    "    # Use Unicode en dash '–' (U+2013), surrounded by spaces as seen in data\n",
    "    split_cols = df['store_name'].str.split(' – ', n=1, expand=True)\n",
    "\n",
    "    # Ensure there are always 2 columns\n",
    "    if split_cols.shape[1] == 1:\n",
    "        split_cols[1] = None  # add a second column if needed\n",
    "\n",
    "    # Replace missing values with 'All Stores'\n",
    "    split_cols = split_cols.fillna('All Stores')\n",
    "\n",
    "    # Rename and assign\n",
    "    split_cols.columns = ['country', 'store']\n",
    "    df[['country', 'store']] = split_cols\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_events(df, events):\n",
    "    df = df.merge(events, on='date', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "df = parse_store_name(df)\n",
    "df = merge_events(df, events)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18766.000000</td>\n",
       "      <td>18766</td>\n",
       "      <td>18766</td>\n",
       "      <td>18766.000000</td>\n",
       "      <td>18766</td>\n",
       "      <td>18766</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California</td>\n",
       "      <td>All Stores</td>\n",
       "      <td>SuperBowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6824</td>\n",
       "      <td>1706</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-30 12:00:00</td>\n",
       "      <td>43101.355174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-29 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-03-30 00:00:00</td>\n",
       "      <td>17746.470000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-30 12:00:00</td>\n",
       "      <td>23064.675000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-31 00:00:00</td>\n",
       "      <td>31238.132500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-30 00:00:00</td>\n",
       "      <td>394304.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.162362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64141.029008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store_id  store_name                 date        revenue  \\\n",
       "count   18766.000000       18766                18766   18766.000000   \n",
       "unique           NaN          11                  NaN            NaN   \n",
       "top              NaN  All Stores                  NaN            NaN   \n",
       "freq             NaN        1706                  NaN            NaN   \n",
       "mean        5.000000         NaN  2013-05-30 12:00:00   43101.355174   \n",
       "min         0.000000         NaN  2011-01-29 00:00:00       0.000000   \n",
       "25%         2.000000         NaN  2012-03-30 00:00:00   17746.470000   \n",
       "50%         5.000000         NaN  2013-05-30 12:00:00   23064.675000   \n",
       "75%         8.000000         NaN  2014-07-31 00:00:00   31238.132500   \n",
       "max        10.000000         NaN  2015-09-30 00:00:00  394304.000000   \n",
       "std         3.162362         NaN                  NaN   64141.029008   \n",
       "\n",
       "           country       store      event  \n",
       "count        18766       18766       1507  \n",
       "unique           4          11         34  \n",
       "top     California  All Stores  SuperBowl  \n",
       "freq          6824        1706         55  \n",
       "mean           NaN         NaN        NaN  \n",
       "min            NaN         NaN        NaN  \n",
       "25%            NaN         NaN        NaN  \n",
       "50%            NaN         NaN        NaN  \n",
       "75%            NaN         NaN        NaN  \n",
       "max            NaN         NaN        NaN  \n",
       "std            NaN         NaN        NaN  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_all = df.groupby('date')['revenue'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "df_all.plot(title='Total Revenue Over Time (All Stores)')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df.groupby(['date', 'country'])['revenue'].sum().unstack()\n",
    "\n",
    "df_country.plot(figsize=(12, 6), title='Revenue Over Time by Country')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out \"All Stores\" label\n",
    "df_filtered = df[df['store'] != 'All Stores']\n",
    "\n",
    "# Group by date and store, then pivot for plotting\n",
    "df_store = df_filtered.groupby(['date', 'store'])['revenue'].sum().unstack()\n",
    "\n",
    "# Plot all stores (excluding \"All Stores\")\n",
    "plt.figure(figsize=(14, 7))\n",
    "df_store.plot(title='Revenue Over Time by Store (excluding \"All Stores\")', figsize=(14, 7))\n",
    "plt.ylabel('Revenue')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Store', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Revenue on Event vs. Non-Event Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df['is_event'] = df['event'].notna()\n",
    "\n",
    "# Aggregate total revenue per day\n",
    "daily_revenue = df.groupby(['date', 'is_event'])['revenue'].sum().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=daily_revenue, x='is_event', y='revenue')\n",
    "plt.title(\"Revenue Distribution on Event vs Non-Event Days\")\n",
    "plt.xticks([0, 1], ['No Event', 'Event'])\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_by_day = df.groupby('date')['revenue'].sum()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "revenue_by_day.plot(label='Revenue')\n",
    "plt.title(\"Revenue Over Time with Event Markers\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "\n",
    "# Mark events on the plot\n",
    "event_dates = events['date']\n",
    "for d in event_dates:\n",
    "    plt.axvline(d, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis of event significance\n",
    "print(\"=== EVENT SIGNIFICANCE ANALYSIS ===\")\n",
    "\n",
    "# 1. Revenue comparison by event type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Prepare data for event analysis\n",
    "df_with_events = df.copy()\n",
    "df_with_events['event_type'] = df_with_events['event'].fillna('No Event')\n",
    "\n",
    "# 1a. Average revenue by event type\n",
    "event_revenue = df_with_events.groupby('event_type')['revenue'].agg(['mean', 'sum', 'count']).reset_index()\n",
    "event_revenue = event_revenue.sort_values('mean', ascending=False)\n",
    "\n",
    "axes[0,0].bar(range(len(event_revenue)), event_revenue['mean'])\n",
    "axes[0,0].set_title('Average Revenue by Event Type')\n",
    "axes[0,0].set_ylabel('Average Revenue')\n",
    "axes[0,0].set_xlabel('Event Type')\n",
    "axes[0,0].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "axes[0,0].set_xticks(range(len(event_revenue)))\n",
    "axes[0,0].set_xticklabels(event_revenue['event_type'], rotation=45, ha='right')\n",
    "\n",
    "# 1b. Total revenue impact by event type\n",
    "axes[0,1].bar(range(len(event_revenue)), event_revenue['sum'])\n",
    "axes[0,1].set_title('Total Revenue by Event Type')\n",
    "axes[0,1].set_ylabel('Total Revenue')\n",
    "axes[0,1].set_xlabel('Event Type')\n",
    "axes[0,1].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "axes[0,1].set_xticks(range(len(event_revenue)))\n",
    "axes[0,1].set_xticklabels(event_revenue['event_type'], rotation=45, ha='right')\n",
    "\n",
    "# 1c. Event frequency\n",
    "event_counts = df_with_events['event_type'].value_counts()\n",
    "axes[1,0].bar(range(len(event_counts)), event_counts.values)\n",
    "axes[1,0].set_title('Event Frequency')\n",
    "axes[1,0].set_ylabel('Number of Occurrences')\n",
    "axes[1,0].set_xlabel('Event Type')\n",
    "axes[1,0].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "axes[1,0].set_xticks(range(len(event_counts)))\n",
    "axes[1,0].set_xticklabels(event_counts.index, rotation=45, ha='right')\n",
    "\n",
    "# 1d. Revenue lift calculation (% change from baseline - BOTH positive and negative)\n",
    "baseline_revenue = df_with_events[df_with_events['event_type'] == 'No Event']['revenue'].mean()\n",
    "event_revenue['revenue_lift'] = ((event_revenue['mean'] - baseline_revenue) / baseline_revenue * 100)\n",
    "# Sort by absolute impact to show both positive and negative effects\n",
    "event_revenue_sorted = event_revenue[event_revenue['event_type'] != 'No Event'].sort_values('revenue_lift', ascending=False)\n",
    "\n",
    "# Create color map: green for positive, red for negative\n",
    "colors = ['green' if x > 0 else 'red' for x in event_revenue_sorted['revenue_lift']]\n",
    "\n",
    "axes[1,1].bar(range(len(event_revenue_sorted)), event_revenue_sorted['revenue_lift'], color=colors, alpha=0.7)\n",
    "axes[1,1].set_title('Revenue Impact by Event Type (% change from baseline)')\n",
    "axes[1,1].set_ylabel('Revenue Change (%)')\n",
    "axes[1,1].set_xlabel('Event Type')\n",
    "axes[1,1].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "axes[1,1].set_xticks(range(len(event_revenue_sorted)))\n",
    "axes[1,1].set_xticklabels(event_revenue_sorted['event_type'], rotation=45, ha='right')\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.8, linewidth=2)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ALL events with their directional impact\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVENT IMPACT ANALYSIS - BOTH POSITIVE AND NEGATIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate positive and negative impacts\n",
    "positive_events = event_revenue_sorted[event_revenue_sorted['revenue_lift'] > 0]\n",
    "negative_events = event_revenue_sorted[event_revenue_sorted['revenue_lift'] < 0]\n",
    "neutral_events = event_revenue_sorted[event_revenue_sorted['revenue_lift'] == 0]\n",
    "\n",
    "print(f\"\\n🟢 POSITIVE IMPACT EVENTS ({len(positive_events)} events):\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in positive_events.iterrows():\n",
    "    print(f\"{row['event_type']:25s}: {row['revenue_lift']:+6.1f}% lift (Avg: ${row['mean']:,.0f}, n={row['count']})\")\n",
    "\n",
    "if len(negative_events) > 0:\n",
    "    print(f\"\\n🔴 NEGATIVE IMPACT EVENTS ({len(negative_events)} events):\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in negative_events.iterrows():\n",
    "        print(f\"{row['event_type']:25s}: {row['revenue_lift']:+6.1f}% drop (Avg: ${row['mean']:,.0f}, n={row['count']})\")\n",
    "\n",
    "if len(neutral_events) > 0:\n",
    "    print(f\"\\n⚪ NEUTRAL IMPACT EVENTS ({len(neutral_events)} events):\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in neutral_events.iterrows():\n",
    "        print(f\"{row['event_type']:25s}: {row['revenue_lift']:+6.1f}% change (Avg: ${row['mean']:,.0f}, n={row['count']})\")\n",
    "\n",
    "print(f\"\\n📊 BASELINE (No Event): ${baseline_revenue:,.0f}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_positive_lift = positive_events['revenue_lift'].sum()\n",
    "total_negative_impact = negative_events['revenue_lift'].sum() if len(negative_events) > 0 else 0\n",
    "print(f\"\\n📈 SUMMARY:\")\n",
    "print(f\"   Total Positive Lift: {total_positive_lift:+.1f}%\")\n",
    "print(f\"   Total Negative Impact: {total_negative_impact:+.1f}%\")\n",
    "print(f\"   Net Event Impact: {total_positive_lift + total_negative_impact:+.1f}%\")\n",
    "\n",
    "# Print the 10 MOST SIGNIFICANT events (highest absolute impact)\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 TOP 10 MOST SIGNIFICANT EVENTS (Highest Absolute Impact)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by absolute impact to get most significant events\n",
    "event_revenue_abs_sorted = event_revenue_sorted.copy()\n",
    "event_revenue_abs_sorted['abs_impact'] = abs(event_revenue_abs_sorted['revenue_lift'])\n",
    "top_10_significant = event_revenue_abs_sorted.nlargest(10, 'abs_impact')\n",
    "\n",
    "for i, (idx, row) in enumerate(top_10_significant.iterrows(), 1):\n",
    "    impact_direction = \"📈\" if row['revenue_lift'] > 0 else \"📉\"\n",
    "    impact_word = \"BOOST\" if row['revenue_lift'] > 0 else \"DROP\"\n",
    "    print(f\"{i:2d}. {impact_direction} {row['event_type']:25s}: {row['revenue_lift']:+7.1f}% {impact_word} \"\n",
    "          f\"(Avg: ${row['mean']:7,.0f}, n={row['count']:2d})\")\n",
    "\n",
    "print(f\"\\n📊 For comparison, baseline revenue: ${baseline_revenue:,.0f}\")\n",
    "print(f\"💡 These events should be HIGH PRIORITY for your forecasting models!\")\n",
    "\n",
    "# 2. Enhanced Box plot showing revenue distribution - including negative events\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Select most impactful events (both positive and negative)\n",
    "top_positive = positive_events.head(5)['event_type'].tolist()\n",
    "top_negative = negative_events.head(5)['event_type'].tolist() if len(negative_events) > 0 else []\n",
    "significant_events = top_positive + top_negative + ['No Event']\n",
    "\n",
    "df_plot = df_with_events[df_with_events['event_type'].isin(significant_events)]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplot with two plots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Box plot of revenue distribution\n",
    "sns.boxplot(data=df_plot, x='event_type', y='revenue', ax=ax1)\n",
    "ax1.set_title('Revenue Distribution by Event Type (Most Impactful Events + Baseline)')\n",
    "ax1.set_xlabel('Event Type')\n",
    "ax1.set_ylabel('Revenue')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add baseline line\n",
    "ax1.axhline(y=baseline_revenue, color='red', linestyle='--', alpha=0.7, label=f'Baseline: ${baseline_revenue:,.0f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Revenue impact waterfall chart\n",
    "event_impacts = event_revenue_sorted['revenue_lift'].values\n",
    "event_names = event_revenue_sorted['event_type'].values\n",
    "colors_waterfall = ['green' if x > 0 else 'red' for x in event_impacts]\n",
    "\n",
    "bars = ax2.bar(range(len(event_impacts)), event_impacts, color=colors_waterfall, alpha=0.7)\n",
    "ax2.set_title('Event Impact Waterfall - All Events (% Change from Baseline)')\n",
    "ax2.set_ylabel('Revenue Change (%)')\n",
    "ax2.set_xlabel('Event Type')\n",
    "ax2.set_xticks(range(len(event_impacts)))\n",
    "ax2.set_xticklabels(event_names, rotation=45, ha='right')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.8, linewidth=2)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, impact in zip(bars, event_impacts):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -1),\n",
    "             f'{impact:+.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the TOP 10 most significant events from our analysis\n",
    "TOP_SIGNIFICANT_EVENTS = [\n",
    "    'Christmas',\n",
    "    'Thanksgiving', \n",
    "    'NewYear',\n",
    "    'Halloween',\n",
    "    'OrthodoxEaster',\n",
    "    'LaborDay',\n",
    "    'SuperBowl'\n",
    "]\n",
    "\n",
    "# Additional compound events (treat as single features)\n",
    "COMPOUND_EVENTS = [\n",
    "    'OrthodoxEaster, Easter',\n",
    "    'OrthodoxEaster, Cinco De Mayo',\n",
    "    'NBAFinalsEnd, Father\\'s day'\n",
    "]\n",
    "\n",
    "def fit_enhanced_sarimax(df, store_id, forecast_steps=30):\n",
    "    \"\"\"Enhanced SARIMAX with top significant events\"\"\"\n",
    "    \n",
    "    # Prepare data for specific store\n",
    "    store_data = df[df['store_id'] == store_id].copy()\n",
    "    \n",
    "    # Ensure we have a complete date range (fill missing dates with 0 revenue)\n",
    "    date_range = pd.date_range(start=store_data['date'].min(), \n",
    "                              end=store_data['date'].max(), \n",
    "                              freq='D')\n",
    "    store_data = store_data.set_index('date').reindex(date_range, fill_value=0)\n",
    "    \n",
    "    # Forward fill the categorical columns\n",
    "    for col in ['country', 'store', 'event']:\n",
    "        if col in store_data.columns:\n",
    "            store_data[col] = store_data[col].fillna(method='ffill').fillna('None')\n",
    "    \n",
    "    # Create binary indicators for TOP significant events\n",
    "    exog_cols = []\n",
    "    \n",
    "    # Add individual significant events\n",
    "    for event_name in TOP_SIGNIFICANT_EVENTS:\n",
    "        col_name = f'is_{event_name}'\n",
    "        store_data[col_name] = (store_data['event'] == event_name).astype(float)\n",
    "        exog_cols.append(col_name)\n",
    "    \n",
    "    # Add compound events (handle exact matches)\n",
    "    for compound_event in COMPOUND_EVENTS:\n",
    "        col_name = f'is_{compound_event.replace(\", \", \"_\").replace(\"\\'\", \"\")}'\n",
    "        store_data[col_name] = (store_data['event'] == compound_event).astype(float)\n",
    "        exog_cols.append(col_name)\n",
    "    \n",
    "    # Add general event indicator for any other events\n",
    "    known_events = TOP_SIGNIFICANT_EVENTS + COMPOUND_EVENTS\n",
    "    store_data['has_other_event'] = (\n",
    "        (store_data['event'].notna()) & \n",
    "        (store_data['event'] != 'None') & \n",
    "        (~store_data['event'].isin(known_events))\n",
    "    ).astype(float)\n",
    "    exog_cols.append('has_other_event')\n",
    "    \n",
    "    # Select exogenous features\n",
    "    exog_features = store_data[exog_cols]\n",
    "    \n",
    "    try:\n",
    "        # Fit SARIMAX with enhanced features\n",
    "        model = SARIMAX(\n",
    "            endog=store_data['revenue'],\n",
    "            exog=exog_features,\n",
    "            order=(1,1,1),\n",
    "            seasonal_order=(0,1,1,7),  # Weekly seasonality\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        result = model.fit(disp=False, maxiter=150)\n",
    "        \n",
    "        # Create future exogenous variables\n",
    "        future_index = pd.date_range(store_data.index[-1] + pd.Timedelta(days=1), \n",
    "                                   periods=forecast_steps, freq='D')\n",
    "        future_exog = pd.DataFrame(0, index=future_index, columns=exog_features.columns)\n",
    "        \n",
    "        # Set known future events (you can customize this section)\n",
    "        # Example: Christmas 2015\n",
    "        # future_exog.loc['2015-12-25', 'is_Christmas'] = 1\n",
    "        \n",
    "        # Forecast\n",
    "        forecast = result.get_forecast(steps=forecast_steps, exog=future_exog)\n",
    "        \n",
    "        # Print feature importance (coefficients)\n",
    "        print(f\"\\nSARIMAX Feature Coefficients for Store {store_id}:\")\n",
    "        \n",
    "        # Debug: Print all available parameter names\n",
    "        print(\"Available parameters:\", list(result.params.index))\n",
    "        \n",
    "        # Try to access exogenous coefficients using different naming conventions\n",
    "        exog_params = [param for param in result.params.index if 'x' in param.lower() or any(col.lower() in param.lower() for col in exog_features.columns)]\n",
    "        \n",
    "        if exog_params:\n",
    "            print(\"Found exogenous parameters:\", exog_params)\n",
    "            for i, col in enumerate(exog_features.columns):\n",
    "                if i < len(exog_params):\n",
    "                    coef = result.params[exog_params[i]]\n",
    "                    print(f\"  {col:25s}: {coef:8.1f}\")\n",
    "                else:\n",
    "                    print(f\"  {col:25s}: parameter not found\")\n",
    "        else:\n",
    "            print(\"No exogenous parameters found in model\")\n",
    "        \n",
    "        return forecast.summary_frame(), result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Enhanced SARIMAX failed for store {store_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "# Test enhanced models on a single store first\n",
    "print(\"🚀 TESTING ENHANCED MODELS WITH TOP 10 SIGNIFICANT EVENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Testing on store_id = 1:\")\n",
    "\n",
    "print(\"\\n=== ENHANCED SARIMAX Results ===\")\n",
    "sarima_forecast, sarima_model = fit_enhanced_sarimax(df, 1)\n",
    "if sarima_forecast is not None:\n",
    "    print(\"\\nForecast Preview:\")\n",
    "    print(sarima_forecast[['mean', 'mean_ci_lower', 'mean_ci_upper']].head())\n",
    "else:\n",
    "    print(\"SARIMAX model failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
